% gjr.tex
\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % support the \includegraphics command and optio
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\newcommand{\Var}{\mathop{\rm{Var}}}
\newcommand{\Cov}{\mathop{\rm{Cov}}}
\theoremstyle{remark}
\newtheorem*{example}{}

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

\title{Generalized Jarrow-Rudd}
\author{Keith A. Lewis}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
The Black-Sholes/Merton model has been Nobel prize winning successful,
but parameterizing models that fit market data has been a remarkably
recalcitrant problem. This note explores using cumulants of the
log of the the risk-neutral distribution of the underlying to
achieve this.

Instead of working with the difference of the cumulants from a lognormal
distribution as in Jarrow-Rudd \cite{JarRud1982} we work with the difference of
the cumulants from a normal distribution. 
This is more natural since the cumulants of a normal distribution are
easier to interpret.

Just as with moments, not every cumulant sequence corresponds to a
random variable. In the case of infinitely divisible random variables
having finite variance, there is a condition similar to the Hamburger
moment problem that ensures this.
\end{abstract}

\section{Outline}
In section 2 we recall some basic fact about cumulants.
Section 3 uses an Edgeworth expansion to give an explicit formula for
the cumulative distribution function of the perturbed distribution.
Option valuation is considered in section 4. We show how the
Esscher transform can be calculated using the same method as in
section 3. The last section collects some
general remarks.

\section{Cumulants}

The {\em cumulants}, \((\kappa_n)\), of a random variable \(X\)
are defined by
\[
\kappa(u) = \log Ee^{uX} = \sum_{n=0}^\infty \kappa_n \frac{u^n}{n!}
\]
Taking \(u = 0\) yields \(\kappa_0 = 0\). Since
\((d/du)^n\kappa(u)|_{u = 0} = \kappa_n\) it is easy to
work out that
\(\kappa_1 = EX\) and \(\kappa_2 = \Var(X)\). Higher order
cumulants are less intuitive.

Cumulants have some handy properties. 
The cumulants of a random variable plus a constant are the 
same except the first cumulant is increased by the constant.
More generally, the cumulants of the sum of two independent 
random variables are the sums of their cumulants.
They scale homogeneously, the \(n\)-th cumulant of a constant
times a random variable is
\(\kappa_n(cX) = c^n\kappa_n(X)\).

Another nice property of cumulants is that they are more likely
to exists than moments, \(m_n = EX^n\).
The relationship between cumulants and moments involves Bell
polynomials\cite{Bel1934}.
\[
Ee^{uX} = \sum_{n=0}^\infty m_n \frac{u^n}{n!}
 = \exp(\sum_{n=1}^\infty \kappa_n \frac{u^n}{n!})
= \sum_{n=0}^\infty B_n(\kappa_1,\dots,\kappa_n) \frac{u^n}{n!}
\]
where \(B_n(\kappa_1,\dots,\kappa_n)\) is the \(n\)-th complete
Bell polynomial.
This is just a special case of the
Fa\`a di Bruno formula first proved by Louis Fran\c{c}ois Antoine
Arborgast in 1800\cite{Arb1800}.
Bell polynomials satisfy the recurrence \cite{Com1974} \(B_0 = 1\) and
\[
B_{n+1}(x_1,\dots,x_{n+1}) = \sum_{k=0}^n \binom{n}{k}
B_{n - k}(x_1,\dots, x_{n - k}) x_{k+1}.
\]

\subsection{Examples}
\begin{example}[Normal]
If \(X\) is normal then \(Ee^X = e^{E X + \Var(X)/2}\) so
\(Ee^{uX} = e^{uEX + u^2\Var(X)/2}\) showing the
third and higher order cumulants vanish. If the cumulants of
a random variable vanish after some some point, then it must
be normal\cite{Luk1970} (Theorem 7.3.5).
Something to keep in mind with computer
implementations.
\end{example}
\begin{example}[Poisson]
If \(X\) is Poisson
with mean \(\mu\) then \(\kappa_n = \mu\) for all \(n\).
\end{example}
%\begin{example}[Compound Poisson]
%If \(Y\) is Poisson with mean \(\mu\) and \(Z_j\) are
%independent and identically distributed, define
%\(X = \sum_{j=1}^{Y} Z_j\). The cumulants of \(X\)
%are \(\kappa_n = ?\).
%\end{example}
%\begin{align*}
%Ee^{uX} &= \sum_{k=0}^\infty Ee^{u(Z_1 + \cdots Z_k)}\mu^k/k!\,e^{-\mu}\\
%&= \sum_{k=0}^\infty (Ee^{u Z_1})^k\mu^k/k!\,e^{-\mu}\\
%&= \sum_{k=0}^\infty (Ee^{u Z_1}\mu)^k/k!\,e^{-\mu}\\
%&= e^{\mu Ee^{u Z_1}}\,e^{-\mu}\\
%&= e^{\mu (Ee^{u Z_1} - 1)}\\
%\end{align*}
%Define \(\lambda(u) = \log Ee^{uZ_1}\). Then
%\(\log E e^{uX} = \mu(e^{\lambda(u)} -1)\).
\begin{example}[Exponential]
If \(X\) is exponential with mean \(\mu\) then
\(\kappa_n = (n - 1)!\mu^n\).
\end{example}
\begin{example}[Gamma]
If \(X\) is gamma with mean \(\alpha/\beta\) and variance
\(\alpha/\beta^2\) then \(\kappa_n = (n - 1)!\alpha/\beta^n\).
Note gamma is exponential when
\(\alpha = 1\) and \(\beta = 1/\mu\).
%\begin{align*}
%Ee^{uX} &= \int_0^\infty e^{ux} x^{\alpha-1} e^{-\beta x}
%\frac{\beta^\alpha}{\Gamma(\alpha)}\,dx\\
%&= \int_0^\infty x^{\alpha-1} e^{-(\beta -u) x}
%\frac{\beta^\alpha}{\Gamma(\alpha)}\,dx\\
%&= \frac{\Gamma(\alpha)}{(\beta - u)^\alpha} \frac{\beta^\alpha}{\Gamma(\alpha)}\\
%&= (1 - u/\beta)^{-\alpha}\\
%\end{align*}
%so
%\(
%\log E e^{uX} = \alpha \sum_{n=1}^\infty (u/\beta)^n/n\).


\end{example}
%\(\chi^2\) with \(r\) degrees of freedom \(\kappa_n = 2^{n-1}(n-1)!r\).

\section{Edgeworth Expansion}
Given random
variables \(X\) and \(Y\), we have
\[\log E e^{iuY} - \log E e^{iuX} = \sum_{n=1}^\infty \Delta\kappa_n (iu)^n/n!\]
where \((\Delta\kappa_n)\) are the differences of the cumulants 
of \(X\) and \(Y\), so
\[
Ee^{iuY} = Ee^{iuX}\sum_{n=0}^\infty B_n(\Delta\kappa_1,...,\Delta\kappa_n)(iu)^n/n!.
\]

Let \(F\) and \(G\) be the cumulative distribution functions of
\(X\) and \(Y\) respectively.
Since the Fourier transform of \(F'\) is \(-iu \hat F(u)\),
the Fourier transform of the \(n\)-th derivative
\(F^{(n)}\) is \((-iu)^n\hat F(u)\).
Taking the inverse Fourier transform shows
\[
G(x) = \sum_{n=0}^\infty (-1)^n \frac{B_n}{n!} F^{(n)}(x)
\]

\subsection{Hermite Polynomials}
The derivatives the standard normal cumulative distribution 
can be computed using Hermite polynomials\cite{AbrSte1964}
pp. 793--801.
Recall the Hermite polynomials are the result of the Gram-Schmidt
process applied to the basis \((x^n)_{n\ge0}\) on the Hilbert space
having inner product \((f,g) = \int_{-\infty}^\infty f(x)g(x)\,e^{-x^2/2}dx\).
The standard results are
\[
H_n(x) = (-1)^n e^{x^2/2}\frac{d^n}{dx^n}e^{-x^2/2}
\]
and they satisfy the recurrence \(H_0(x) = 1\), \(H_1(x) = x\) and
\[
H_{n+1}(x) = xH_n(x) - n H_{n-1}(x).
\]
Note some authors use \(He_n(x)\) instead of \(H_n(x)\).

%We have
%\(
%F^{(1)}(x) = F'(x) = e^{-x^2/2}/\sqrt{2\pi} 
%= H_0(x) e^{-x^2/2}/\sqrt{2\pi}.
%\)
%More generally, 
%\[
%F^{(n)}(x) = (-1)^{n-1} H_{n-1}(x) e^{-x^2/2}/\sqrt{2\pi}.
%\]
%
%For appropriate \(\phi\), e.g., exponentially bounded, 
%integration by parts shows 
%\(\int_{-\infty}^\infty \phi(x)\,dF^{(n)}
%= - \int_{-\infty}^\infty \phi'(x)F^{(n)}\,dx
%= - \int_{-\infty}^\infty \phi'(x)\,dF^{(n-1)}\)
%so for any \(k\le n\)
%\[
%\int_{-\infty}^\infty \phi(x)\,dF^{(n)}
%= (-1)^k \int_{-\infty}^\infty \phi^{(k)}(x)\,dF^{(n-k)}.
%\]

%\subsection{Put Valuation}
%The forward value of a put option with strike \(k\) is
%\(E\max\{k - S, 0\}\) where 
%\(S = s\exp(-\kappa(t,\sigma) + \sigma\sqrt{t}Y)\)
%and \(\kappa(t\sigma) = \log E \exp(\sigma\sqrt{t}Y)\).
%so \(ES = s\).
%
%Let \(\phi(y) = \max\{k - se\exp(-\kappa(t,\sigma) + \sigma\sqrt{t}y)\)

\section{Option Valuation}
The forward value of a put on a stock \(S\) with strike \(k\) is
\[
E\max\{k - S,0\} = k P(S \le k) - E S\,P^*(S \le k)
\]
where \(dP^*/dP = S/ES\) is the {\em Esscher transform}\cite{Ess1932}.
This is the analog of 
\(E\max\{k - S,0\} = kN(-d_2) - sN(-d_1)\)
when \(S\) is lognormal.

\subsection{Black Model}
The Black model is \(S = S_t = se^{-\sigma^2t/2 + \sigma B_t}\)
where \((B_t)\) is standard Brownian motion and \(t\) is time
in years. Note \(ES = s\) and \(S \le k\)
if and only if \(B_t/\sqrt{t} \le z\) where
\(z = (\log k/s + \sigma^2 t/2)/\sigma\sqrt{t}\). In the usual
notation, \(z = -d_2\).

Letting \(X = B_t/\sqrt{t}\) and so \(F = N\) is the standard
normal cumulative distributing function we have
\(P(X\le z) = F(z)\) and \(P^*(X\le z) = P(X + \sigma\sqrt{t} \le z)
= F(z - \sigma\sqrt{t})\) where we use the fact
\[
Ee^U f(V) = Ee^U Ef(V + \Cov(U,V))
\]
if \(U\) and \(V\) are
jointly normal. In the standard notation \(z - \sigma\sqrt{t} = -d_1\).

\subsection{Generalized Black Model}
The generalized Black model is \(S = se^{-\kappa(t,\sigma) + \sigma X_t}\)
where \((X_t)\) is any stochastic process and 
\(\kappa(t, \sigma) = \log E \exp(\sigma X_t)\). 
Note \(ES = s\) and \(S \le k\)
if and only if 
\(X_t/\sqrt{t} \le (\log k/s + \kappa(t,\sigma))/\sigma\sqrt{t}\).

Letting \(Y = X_t/\sqrt{t}\) and \(G\) be the corresponding cumulative
distribution function, \(G(z) = P(Y\le z)\), we need to compute
\[
G^*(z) = P^*(Y\le z) = Ee^{-\kappa(t,\sigma) + \sigma\sqrt{t}Y}1(Y\le z).
\]
Letting \(\gamma = \sigma\sqrt{t}\), the cumulants of \(G^*\) can be found from
\begin{align*}
\log E e^{uY^*} &= \log E e^{-\kappa(t,\gamma) + \gamma Y} e^{uY}\\
&= -\kappa(t, \gamma) + \log E e^{(\gamma + u) Y}\\
&= -\sum_{n=1}^\infty \kappa_n \frac{\gamma^n}{n!} 
	+ \kappa_n \frac{(u + \gamma)^n}{n!}\\
&= \sum_{n=1}^\infty \sum_{k=0}^{n-1} \kappa_n \binom{n}{k}
	\frac{\gamma^{n - k}u^k}{n!}\\
&= \sum_{k=1}^\infty \sum_{n=k + 1}^\infty \kappa_n 
	\binom{n}{k}\frac{\gamma^{n - k}u^k}{n!}\\
&= \sum_{k=1}^\infty \sum_{n=k + 1}^\infty \kappa_n \frac{\gamma^{n-k}}{(n-k)!}
	\frac{u^k}{k!}\\
&= \sum_{k=1}^\infty 
	\bigl(\sum_{n=1}^\infty \kappa_{n+k} \frac{\gamma^n}{n!}\bigr)
	\frac{u^k}{k!}\\
\end{align*}
This shows the cumulants of \(Y^*\) are 
\(\kappa^*_k = \sum_{n=1}^\infty \kappa_{n + k}\gamma^n/n!\)
so we can also compute \(P(Y^* \le z) = G^*(z)\) using the same
technique as we did for \(G\).

If \(G\) is standard normal then all cumulants vanish except \(\kappa_2 = 1\). The only nonzero cumulants of \(G^*\) are \(\kappa_1 = \gamma\)
and \(\kappa_2 = 1\). The corresponding reduced Bell polynomials
are \(B_n = \gamma^n/n!\) and we see the above formula is just
the Taylor series expansion of \(G(z - \gamma)\).
%
%The Edgeworth series expands the exponential in a power series.
%\[\exp\bigl(\sum_{n=0}^\infty \Delta\kappa_n (iu)^n/n!\bigr)
%= \sum_{k=0}^\infty (\sum_{n=0}^\infty \Delta\kappa_n (iu)^n/n!)^k/k!\]
%

%\subsection{Computations}
%Explicit formulas the first few Bell polynomials:
%\begin{align*}
%B_0 &= 1\\
%B_1 &= x_1\\
%B_2 &= B_1 x_1 + B_0 x_2\\
%&= x_1^2 + x_2\\
%B_3 &= B_2 x_1 + 2B_1 x_2 + B_0 x_3\\
%&= x_1^3 + x_1 x_2 + 2x_1 x_2 + x_3\\
%&= x_1^3 + 3x_1 x_2 + x_3\\
%B_4 &= B_3 x_1 + 3 B_2 x_2 + 3 B_1 x_3 + B_0 x_4\\
%&= (x_1^3 + 3x_1 x_2 + x_3) x_1  + 3 (x_1^2 + x_2)x_2 + 3 x_1 x_3 + x_4\\
%&= x_1^4 + 6x_1^2 x_2 + 4 x_1 x_3 + 3x_2^2 + x_4\\
%\end{align*}
%Explicit formulas for the first few Hermite polynomials:
%\begin{align*}
%H_0 &= 1\\
%H_1 &= x\\
%H_2 &= x^2 - 1\\
%H_3 &= x^3 - x\\
%H_4 &= x^4 - 6x^2 + 3\\
%H_5 &= x^5 - 10x^3 + 15x\\
%H_6 &= x^6 - 15x^4 + 45x^2 - 15\\
%\end{align*}
%
%Assuming \(\kappa_1 = \kappa_2 = 0\) the first few terms of the Edgeworth expansion are:
%\begin{align*}
%G(x) &= \sum_{n=0}^\infty (-1)^n B_n F^{(n)}(x)/n!\\
%&= F(x) - B_1 F'(x) + B_2F''(x)/2 - B_3F^{(3)}(x)/6 + B_4F^{(4)}/24\\
%&= F(x) + (-\kappa_3 (x^2 - 1)/6 - \kappa_4 (x^3-x)/24)e^{-x^2/2}/\sqrt{2\pi}\\
%\end{align*}
%The distribution is unimodal if and only if the second derivative has exactly one root.
%\begin{align*}
%G''(x) &= \sum_{n=0}^\infty (-1)^n B_n F^{(n+1)}(x)/n!\\
%&= F^{(2)}(x) - B_1 F^{(3)}(x) + B_2F^{(4)}(x)/2 - B_3F^{(5)}(x)/6 + B_4F^{(6)}/24\\
%&= (-x - \kappa_3 (x^4 - 6x^2 + 3)/6
%	 - \kappa_4 (x^5 - 10x^3 + 15x)/24)e^{-x^2/2}/\sqrt{2\pi}\\
%\end{align*}
%

\subsection{L\'evy Processes}

%Let \(X = aN + bP + c\) where \(N\) is standard normal and \(P\)
%is Poisson with mean \(\mu\).

%\(EX = b\mu + c\) and \(\Var X = a^2 + b^2\). Taking
%\(b = \sqrt{1 - a^2}\)

Kolmogorov's version of the L\'evy-Khintchine theorem\cite{Kol1992}
states that if a random variable \(X\) is infinitely divisible
there exists a number \(\gamma\) and a non-decreasing function
\(G\) defined on the real line such that
\[
\kappa(u) = \log Ee^{uX} = u\gamma + \int_{-\infty}^\infty K_u(x)\,dG(x),
\]
where \(K_u(x) = (e^{ux} - 1 - ux)/x^2\). Note the first
cumulant of \(X\) is \(\gamma\) and for \(n\ge 2\),
\(\kappa_n = \int_{-\infty}^\infty x^{n-2}\,dG(x)\). In particular
the variance of \(X\) is 
\(\int_{-\infty}^\infty dG(x) = G(\infty) - G(-\infty)\).

If \((X_t)\) is a L\'evy process then \(X_1\) is
infinitely divisible and \(\log Ee^{uX_t} = t\kappa(u)\).

%The K-model takes \(\gamma = 0\) and \(G\) of the form
%\[
%G(x) =
%\begin{cases}
%a e^{x/\alpha} &x < 0\\
%1 - be^{-x/\beta} & x > 0\\
%\end{cases}
%\]
%Note \(G\) jumps by \(1 - a - b\) at the origin. If \(a = b = 0\)
%this reduces to a standard normal distribution.
%
%The cumulants are simple to compute: \(\kappa_1 = 0\), \(\kappa_2 = 1\),
%and \(\kappa_{n+2} = (a(-\alpha)^n + b\beta^n)n!\) for \(n > 1\).

\section{Remarks}
The Gram-Charlier A series expands the quotients of cumulative
distribution functions \(G/F\) using Hermite polynomials, but does not
have asymptotic convergence, whereas the Edgeworth expansion involves
the quotient of characteristic functions \(\hat G/\hat F\) in terms of
cumulants and does have asymptotic convergence, ignoring some dainty
facts \cite{Pet1975}.

A software implementation in C++ is available at
\url{https://fmsgjr.codeplex.com} and Excel add-ins
at \url{https://xllgjr.codeplex.com}.

\bibliographystyle{amsplain}
\bibliography{gjr}

\end{document}
